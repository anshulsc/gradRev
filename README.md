# gradRev: Understanding Backpropagation in PyTorch with Autograd

**gradrev** is a Python library that provides a simplified implementation of autograd to help understand the mechanics of backpropagation in PyTorch and demystify the inner workings of PyTorch's autograd.

## Introduction

Deep learning and neural networks have become essential tools in various fields, from computer vision to natural language processing. PyTorch is a popular deep learning framework that employs automatic differentiation, also known as autograd, to calculate gradients for backpropagation.

Understanding how autograd works is fundamental to grasp the training process of neural networks. **gradrev** simplifies this complex concept, allowing you to explore the core principles of backpropagation without the need for a deep dive into PyTorch's source code.

```
o = tan(x1w1 + x2w2 + b)
```
![gradev](https://drive.google.com/file/d/1_WqvQP1CcuDrDjAsmV9FKNhxiUiBmV0Z/view?usp=sharing)
